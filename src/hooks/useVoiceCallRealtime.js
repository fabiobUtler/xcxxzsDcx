import { useState, useRef, useCallback } from "react";

export function useVoiceCallRealtime({ onTranscript, onAIResponse }) {
  const [isCalling, setIsCalling] = useState(false);
  const [transcripts, setTranscripts] = useState([]);
  const wsRef = useRef(null);
  const audioContextRef = useRef(null);
  const processorRef = useRef(null);

  const addTranscript = useCallback(
    (who, text) => {
      const entry = { who, text };
      setTranscripts((prev) => [...prev, entry]);
      onTranscript?.(entry);
    },
    [onTranscript]
  );

  const startCall = async () => {
    try {
      const protocol = window.location.protocol === "https:" ? "wss://" : "ws://";
      const wsUrl = `${protocol}${window.location.host}/api/realtime`;
      wsRef.current = new WebSocket(wsUrl);

      console.log("ðŸ“¡ ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº:", wsUrl);

      wsRef.current.binaryType = "arraybuffer"; // âœ… Ð²Ð°Ð¶Ð½Ð¾ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ Ð±Ð¸Ð½Ð°Ñ€ÐºÐ¸

      wsRef.current.onopen = async () => {
        console.log("âœ… Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ñ ÑÐµÑ€Ð²ÐµÑ€Ð¾Ð¼ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾");
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              sampleRate: 24000,
            },
          });

          audioContextRef.current = new AudioContext({ sampleRate: 24000 });
          await audioContextRef.current.resume();

          const source = audioContextRef.current.createMediaStreamSource(stream);
          const processor = audioContextRef.current.createScriptProcessor(2048, 1, 1);
          processorRef.current = processor;

          source.connect(processor);
          processor.connect(audioContextRef.current.destination);

          processor.onaudioprocess = (e) => {
            const inputData = e.inputBuffer.getChannelData(0);
            const samples = new Int16Array(inputData.length);

            for (let i = 0; i < inputData.length; i++) {
              let s = Math.max(-1, Math.min(1, inputData[i]));
              samples[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
            }

            if (wsRef.current?.readyState === WebSocket.OPEN) {
              wsRef.current.send(samples.buffer); // ðŸ”¥ Ð±Ð¸Ð½Ð°Ñ€ÐºÐ° ÑƒÑ…Ð¾Ð´Ð¸Ñ‚ ÐºÐ°Ðº ArrayBuffer
            }
          };

          setIsCalling(true);
        } catch (err) {
          console.error("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°:", err);
          alert("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ");
        }
      };

      wsRef.current.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          console.log("ðŸ“© ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾:", data);

          if (data.type === "transcript") {
            addTranscript("You", data.text);
          }

          if (data.type === "response") {
            addTranscript("AI", data.text);
            onAIResponse?.(data.text);

            if (data.audio) {
              const audio = new Audio(`data:audio/wav;base64,${data.audio}`);
              audio.play().catch(console.error);
            }
          }
        } catch (e) {
          console.warn("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ:", event.data);
        }
      };

      wsRef.current.onerror = (err) => {
        console.error("âŒ WebSocket Ð¾ÑˆÐ¸Ð±ÐºÐ°:", err);
      };

      wsRef.current.onclose = () => {
        console.log("ðŸ”´ Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð¾");
        endCall();
      };
    } catch (err) {
      console.error("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÑ‚Ð°Ñ€Ñ‚Ð° Ð·Ð²Ð¾Ð½ÐºÐ°:", err);
    }
  };

  const endCall = () => {
    processorRef.current?.disconnect();
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close().catch(console.warn);
    }
    if (wsRef.current) {
      wsRef.current.close();
    }
    setIsCalling(false);
    setTranscripts([]);
  };

  return {
    isCalling,
    transcripts,
    startCall,
    endCall,
  };
}
