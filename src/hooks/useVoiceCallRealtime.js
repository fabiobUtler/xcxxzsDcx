// src/hooks/useVoiceCallRealtime.js
import { useState, useRef, useCallback } from "react";

export function useVoiceCallRealtime({ onTranscript, onAIResponse }) {
  const [isCalling, setIsCalling] = useState(false);
  const [transcripts, setTranscripts] = useState([]);
  const wsRef = useRef(null);
  const audioContextRef = useRef(null);
  const processorRef = useRef(null);

  const addTranscript = useCallback(
    (who, text) => {
      const entry = { who, text };
      setTranscripts((prev) => [...prev, entry]);
      onTranscript?.(entry);
    },
    [onTranscript]
  );

  const startCall = async () => {
    try {
      // Ð§ÐµÑ€ÐµÐ· Vite proxy: /api/realtime â†’ ws://localhost:3001
      wsRef.current = new WebSocket("ws://localhost:5173/api/realtime");

      wsRef.current.onopen = async () => {
        console.log("âœ… Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ñ ÑÐµÑ€Ð²ÐµÑ€Ð¾Ð¼ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾");
      
            // Ð”Ð¾Ð±Ð°Ð²ÑŒ ÑÑ‚Ð¸ Ð»Ð¾Ð³Ð¸!
      wsRef.current.onerror = (err) => {
        console.error("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° WebSocket (Ð²Ð½ÑƒÑ‚Ñ€Ð¸ startCall):", err);
      };

      wsRef.current.onclose = () => {
        console.log("ðŸ”´ Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð¾ (Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾, Ð¾ÑˆÐ¸Ð±ÐºÐ°)");
      };

        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              sampleRate: 24000,
            },
          });

          audioContextRef.current = new AudioContext({
            sampleRate: 24000,
          });
          const source = audioContextRef.current.createMediaStreamSource(stream);
          const processor = audioContextRef.current.createScriptProcessor(2048, 1, 1);
          processorRef.current = processor;

          source.connect(processor);
          processor.connect(audioContextRef.current.destination);

          processor.onaudioprocess = (e) => {
            const inputData = e.inputBuffer.getChannelData(0);
            const samples = new Int16Array(inputData.length);

            for (let i = 0; i < inputData.length; i++) {
              let s = Math.max(-1, Math.min(1, inputData[i]));
              samples[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
            }

            if (wsRef.current?.readyState === WebSocket.OPEN) {
              wsRef.current.send(samples.buffer);
            }
          };

          setIsCalling(true);
        } catch (err) {
          console.error("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°:", err);
          alert("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ");
        }
      };

      wsRef.current.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);

          if (data.type === "transcript") {
            addTranscript("You", data.text);
          }

          if (data.type === "response") {
            addTranscript("AI", data.text);
            onAIResponse?.(data.text);

            if (data.audio) {
              const audio = new Audio(`data:audio/wav;base64,${data.audio}`);
              audio.play().catch(console.error);
            }
          }
        } catch (e) {
          console.warn("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ:", event.data);
        }
      };

      wsRef.current.onerror = (err) => {
        console.error("WebSocket Ð¾ÑˆÐ¸Ð±ÐºÐ°:", err);
      };

      wsRef.current.onclose = () => {
        console.log("Ð¡Ð¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð¾");
        endCall();
      };
    } catch (err) {
      console.error("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÑ‚Ð°Ñ€Ñ‚Ð° Ð·Ð²Ð¾Ð½ÐºÐ°:", err);
    }
  };

  const endCall = () => {
    processorRef.current?.disconnect();
    audioContextRef.current?.close().catch(console.warn);
    wsRef.current?.close();
    setIsCalling(false);
    setTranscripts([]);
  };

  return {
    isCalling,
    transcripts,
    startCall,
    endCall,
  };
}